{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "878e7401",
      "metadata": {
        "id": "878e7401"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOGASA0X/CPC251_Assignment/blob/main/CPC251_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39d43a4e",
      "metadata": {
        "id": "39d43a4e"
      },
      "source": [
        "                             School of Computer Sciences\n",
        "\n",
        "                                Universiti Sains Malaysia\n",
        "\n",
        "                        Academic Session 2024/2025, Semester 2\n",
        "                    CPC251 MACHINE LEARNING AND COMPUTATIONAL INTELLIGENCE\n",
        "                                    ASSIGNMENT I\n",
        "                           Lecturer:Dr. Mohd Halim Mohd Noor\n",
        "                             Submission Date:13 May 2024\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9b28a8",
      "metadata": {
        "id": "2b9b28a8"
      },
      "source": [
        "#### Group Information\n",
        "\n",
        "Group No: Climate6\n",
        "\n",
        "- Member 1: AZAM TAMHEED\n",
        "- Member 2: MUEED HYDER MIR\n",
        "- Member 3: MUHAMMAD HADIF HAIQAL BIN HADLI\n",
        "- Member 4: ZHANG, YUBIAO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0a3c45",
      "metadata": {
        "id": "2c0a3c45"
      },
      "source": [
        "#### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "id": "79b84136",
      "metadata": {
        "id": "79b84136"
      },
      "outputs": [],
      "source": [
        "#%config Completer.use_jedi=False\n",
        "#import all necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dceb4ff",
      "metadata": {
        "id": "5dceb4ff"
      },
      "source": [
        "#### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "id": "a83d38c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "a83d38c1",
        "outputId": "8999357e-2760-40c4-9f20-0f79ef58f450"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.84279427939994,\n        \"min\": -11.048993760561745,\n        \"max\": 5.290626839750607,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          -1.7211960045165613,\n          -4.814073149981028,\n          -3.2834320939533423\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.692197227258485,\n        \"min\": -2.8652356323255157,\n        \"max\": 9.354404206838558,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          3.85629935272662,\n          4.347387280824288,\n          7.921961306511598\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.921690126051986,\n        \"min\": -2.946965534471293,\n        \"max\": 16.15568228996145,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          10.96323748376468,\n          6.737012207910315,\n          12.588694715494816\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.11154342364337,\n        \"min\": -9.128362517597475,\n        \"max\": 12.732956122327714,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9.357931476405517,\n          12.732956122327714,\n          -2.673427505835411\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5728878819287875,\n        \"min\": -0.8812889328243805,\n        \"max\": 11.522672927582793,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          8.310320063328314,\n          6.889287779251044,\n          4.926287207715424\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b368afc3-325c-43b8-93a0-e9275ccea285\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>946</th>\n",
              "      <td>-1.061491</td>\n",
              "      <td>1.044824</td>\n",
              "      <td>16.155682</td>\n",
              "      <td>-8.798228</td>\n",
              "      <td>-0.881289</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>-4.814073</td>\n",
              "      <td>4.347387</td>\n",
              "      <td>6.737012</td>\n",
              "      <td>12.732956</td>\n",
              "      <td>6.889288</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>913</th>\n",
              "      <td>-11.048994</td>\n",
              "      <td>6.139309</td>\n",
              "      <td>6.877816</td>\n",
              "      <td>-4.185473</td>\n",
              "      <td>5.645072</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575</th>\n",
              "      <td>-4.393519</td>\n",
              "      <td>9.354404</td>\n",
              "      <td>-0.852408</td>\n",
              "      <td>-8.418432</td>\n",
              "      <td>10.270078</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>-8.362895</td>\n",
              "      <td>1.274688</td>\n",
              "      <td>10.529544</td>\n",
              "      <td>9.757067</td>\n",
              "      <td>11.522673</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>-3.283432</td>\n",
              "      <td>7.921961</td>\n",
              "      <td>12.588695</td>\n",
              "      <td>-2.673428</td>\n",
              "      <td>4.926287</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>953</th>\n",
              "      <td>5.290627</td>\n",
              "      <td>4.587922</td>\n",
              "      <td>4.607114</td>\n",
              "      <td>10.161200</td>\n",
              "      <td>4.496489</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>2.431025</td>\n",
              "      <td>7.344819</td>\n",
              "      <td>-2.946966</td>\n",
              "      <td>-9.128363</td>\n",
              "      <td>9.425408</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>761</th>\n",
              "      <td>-1.721196</td>\n",
              "      <td>3.856299</td>\n",
              "      <td>10.963237</td>\n",
              "      <td>9.357931</td>\n",
              "      <td>8.310320</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>-6.085463</td>\n",
              "      <td>-2.865236</td>\n",
              "      <td>4.545722</td>\n",
              "      <td>8.533789</td>\n",
              "      <td>5.152932</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b368afc3-325c-43b8-93a0-e9275ccea285')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b368afc3-325c-43b8-93a0-e9275ccea285 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b368afc3-325c-43b8-93a0-e9275ccea285');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bdfec7e4-646d-4556-b479-3af0d2a278a9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bdfec7e4-646d-4556-b479-3af0d2a278a9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bdfec7e4-646d-4556-b479-3af0d2a278a9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            f1        f2         f3         f4         f5  label\n",
              "946  -1.061491  1.044824  16.155682  -8.798228  -0.881289      0\n",
              "409  -4.814073  4.347387   6.737012  12.732956   6.889288      1\n",
              "913 -11.048994  6.139309   6.877816  -4.185473   5.645072      0\n",
              "575  -4.393519  9.354404  -0.852408  -8.418432  10.270078      0\n",
              "505  -8.362895  1.274688  10.529544   9.757067  11.522673      1\n",
              "576  -3.283432  7.921961  12.588695  -2.673428   4.926287      0\n",
              "953   5.290627  4.587922   4.607114  10.161200   4.496489      1\n",
              "103   2.431025  7.344819  -2.946966  -9.128363   9.425408      0\n",
              "761  -1.721196  3.856299  10.963237   9.357931   8.310320      1\n",
              "368  -6.085463 -2.865236   4.545722   8.533789   5.152932      1"
            ]
          },
          "execution_count": 232,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('classification_dataset.csv')\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0f56c6",
      "metadata": {
        "id": "7f0f56c6"
      },
      "source": [
        "#### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "id": "b4024775",
      "metadata": {
        "id": "b4024775"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y_true, y_pred):\n",
        "    # Reshape the labels and predictions to column vectors\n",
        "    y_true = tf.reshape(y_true, [-1, 1])\n",
        "    y_pred = tf.reshape(y_pred, [-1, 1])\n",
        "    # now  Calculate and return the mean sigmoid cross entropy loss\n",
        "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "753a2eca",
      "metadata": {
        "id": "753a2eca"
      },
      "source": [
        "#### Define function to perform prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "id": "bdbf2168",
      "metadata": {
        "id": "bdbf2168"
      },
      "outputs": [],
      "source": [
        "#apply the sigmoid function to x\n",
        "def sigmoid(x):\n",
        "    return tf.sigmoid(x)\n",
        "\n",
        "#apply the RELU functtion to x\n",
        "def relu(x):\n",
        "    return tf.maximum(x, 0)\n",
        "\n",
        "\n",
        "def forward(x, weights, biases):\n",
        "    z1 = tf.add(tf.matmul(x, weights['h']), biases['b1'])\n",
        "    a1 = relu(z1) # apply the relu function to hidden layer\n",
        "    z2 = tf.add(tf.matmul(a1, weights['out']), biases['out'])\n",
        "    return z2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7bc735b",
      "metadata": {
        "id": "e7bc735b"
      },
      "source": [
        "#### Define function for model training\n",
        "Display the training and validation loss values for each epoch of the training loop. The displayed value must be in 6 decimal places.<br>\n",
        "Hint: <br>\n",
        "Use `tf.GradientTape` to compute the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "id": "fe17ccc5",
      "metadata": {
        "id": "fe17ccc5"
      },
      "outputs": [],
      "source": [
        "def train(X, y, weights, biases, learning_rate):\n",
        "    # Use tf.GradientTape() to record operations for automatic differentiation.\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Call the forward function calculating\n",
        "        # the predicted outputs of the network for the current batch of data.\n",
        "        y_pred = forward(X, weights, biases)\n",
        "        # Calculate the loss between the actual labels (y) and predictions (y_pred).\n",
        "\n",
        "        loss = loss_fn(y, y_pred)\n",
        "\n",
        "    # Use the tape to compute gradients of the loss\n",
        "    gradients = tape.gradient(loss, [weights['h'], weights['out'], biases['b1'], biases['out']])\n",
        "\n",
        "    # Update the model weights and biases using the calculated gradients.\n",
        "\n",
        "    weights['h'].assign_sub(learning_rate * gradients[0])\n",
        "    weights['out'].assign_sub(learning_rate * gradients[1])\n",
        "    biases['b1'].assign_sub(learning_rate * gradients[2])\n",
        "    biases['out'].assign_sub(learning_rate * gradients[3])\n",
        "\n",
        "    # Return the loss computed\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "#initialise history dictionary\n",
        "def fit(x_train, y_train, x_val, y_val, x_test, y_test, weights, biases, epochs, learning_rate, batch_size=None):\n",
        "    history = {'train_loss': [], 'val_loss': [], 'test_loss': []}\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = x_train.shape[0]  # Full-batch training\n",
        "    num_batches = int(np.ceil(x_train.shape[0] / batch_size))\n",
        "\n",
        "#training loop for the numbers of epochs specified\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        #mini batch\n",
        "        for batch_index in range(num_batches):\n",
        "            batch_start = batch_index * batch_size\n",
        "            batch_end = min(batch_start + batch_size, x_train.shape[0])\n",
        "            x_batch = x_train[batch_start:batch_end]\n",
        "            y_batch = y_train[batch_start:batch_end]\n",
        "            #train on batch and get loss\n",
        "            batch_loss = train(x_batch, y_batch, weights, biases, learning_rate)\n",
        "            epoch_loss += batch_loss.numpy()\n",
        "\n",
        "\n",
        "#average loss\n",
        "        epoch_loss /= num_batches\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "\n",
        "        # Validation Loss\n",
        "        val_pred = forward(x_val, weights, biases)\n",
        "        val_loss = loss_fn(y_val, val_pred)\n",
        "        history['val_loss'].append(val_loss.numpy())\n",
        "\n",
        "        # Test Loss\n",
        "        test_pred = forward(x_test, weights, biases)\n",
        "        test_loss = loss_fn(y_test, test_pred)\n",
        "        history['test_loss'].append(test_loss.numpy())\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {epoch_loss}, Val Loss: {val_loss.numpy()}, Test Loss: {test_loss.numpy()}')\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f45213",
      "metadata": {
        "id": "28f45213"
      },
      "source": [
        "#### Define the tensors to hold the weights and biases (create the model)\n",
        "Hint: <br>\n",
        "Use `tf.Variable` to create the tensors.<br>\n",
        "Put the tensors in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "id": "a2e2172b",
      "metadata": {
        "id": "a2e2172b"
      },
      "outputs": [],
      "source": [
        "input_size = 5 # number of features (f1\tf2\tf3\tf4\tf5)\n",
        "hidden_size = 10 # number of neurons in the hidden layer\n",
        "output_size = 1 # binary classification\n",
        "\n",
        "weights = {\n",
        "    'h': tf.Variable(tf.random.normal([input_size, hidden_size], dtype=tf.float64)),\n",
        "    'out': tf.Variable(tf.random.normal([hidden_size, output_size], dtype=tf.float64))\n",
        "}\n",
        "# initialising biases\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random.normal([hidden_size], dtype=tf.float64)),\n",
        "    'out': tf.Variable(tf.random.normal([output_size], dtype=tf.float64))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "176badb8",
      "metadata": {
        "id": "176badb8"
      },
      "source": [
        "#### Split the dataset\n",
        "The ratio of training and test is 7:1:2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "id": "5fa1b9b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fa1b9b6",
        "outputId": "58b3fe2a-6301-42e8-829c-783bdaceebf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 5)\n",
            "(1000,)\n",
            "Training set shapes:\n",
            "(700, 5)\n",
            "(700,)\n",
            "\n",
            "Validation set shapes:\n",
            "(100, 5)\n",
            "(100,)\n",
            "\n",
            "Test set shapes:\n",
            "(200, 5)\n",
            "(200,)\n"
          ]
        }
      ],
      "source": [
        "y = df['label']\n",
        "X = df.drop(columns=['label']).values # drop price and convert to numpy array\n",
        "y = y.values # convert to numpy array\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "seed_num = 42  # Using a fixed seed for reproducibility\n",
        "\n",
        "# Splitting data into training, validation, and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=seed_num)\n",
        "\n",
        "# Further splitting training data into training and validation sets (70% training, 15% validation)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=seed_num)\n",
        "\n",
        "# Printing shapes of the splits\n",
        "print(\"Training set shapes:\")\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"\\nValidation set shapes:\")\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "print(\"\\nTest set shapes:\")\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "#cast training to float64\n",
        "X_train = tf.cast(X_train, tf.float64)\n",
        "y_train = tf.cast(y_train, tf.float64)\n",
        "X_val = tf.cast(X_val, tf.float64)\n",
        "y_val = tf.cast(y_val, tf.float64)\n",
        "X_test = tf.cast(X_test, tf.float64)\n",
        "y_test = tf.cast(y_test, tf.float64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c4d6cf",
      "metadata": {
        "id": "c3c4d6cf"
      },
      "source": [
        "#### Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "id": "J2p3Zt4MHklh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2p3Zt4MHklh",
        "outputId": "9414174a-be77-4ce9-88b0-ac0707fb6757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First five rows of X_train:\n",
            "[[-0.30862009  1.73633873 -0.54617521 -1.40212322  1.50921555]\n",
            " [ 0.9654205   0.18504751  0.71275703  1.50488119 -0.36517122]\n",
            " [-1.64940099  1.12708821 -0.89683306 -0.71355991  0.87133282]\n",
            " [-1.01448595 -0.29597115  0.5875384   0.81855965  1.45268112]\n",
            " [-0.48516687 -0.99146083 -0.58626376  1.27840301 -1.39181021]]\n",
            "\n",
            "First five rows of X_vald (validation set):\n",
            "[[ 1.70615296 -0.27509278  1.11734278  0.9485441  -0.37665605]\n",
            " [-1.45684648 -0.91357976 -0.46527879  0.51412151  0.76533131]\n",
            " [-0.32046422 -0.13542697  1.11941925  0.80634604  0.42593299]\n",
            " [ 0.49951525  0.80270316 -0.40498249 -0.58460573  0.31303596]\n",
            " [ 0.70146495  0.0172996   0.58841737 -0.94947164 -1.02256425]]\n",
            "\n",
            "First five rows of X_test:\n",
            "[[ 1.40782537 -0.87529318 -0.10374545  1.06761466 -0.74705924]\n",
            " [-0.11680952 -0.35992692  1.02053366  0.27115266 -1.68572348]\n",
            " [ 0.11085255 -0.0751017   1.71146636  0.09432908 -1.6363604 ]\n",
            " [ 1.77295172  0.12799389  1.68145856  1.69338922 -2.49776163]\n",
            " [-0.87596014 -1.45434558  0.84705779  0.1705499   0.56727451]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_vald = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Print the first five rows of each dataset\n",
        "print(\"First five rows of X_train:\")\n",
        "print(X_train[:5])\n",
        "\n",
        "print(\"\\nFirst five rows of X_vald (validation set):\")\n",
        "print(X_vald[:5])\n",
        "\n",
        "print(\"\\nFirst five rows of X_test:\")\n",
        "print(X_test[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a2e7d6",
      "metadata": {
        "id": "b1a2e7d6"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "id": "knz9cDRSs6HA",
      "metadata": {
        "id": "knz9cDRSs6HA"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 300\n",
        "batch_size = None  # Full-batch training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "id": "6304c496",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6304c496",
        "outputId": "cd4838e0-1dbf-44fb-89de-ad7f2922888a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 3.227564693221653, Val Loss: 21.243434308123398, Test Loss: 3.3057219106012816\n",
            "Epoch 2, Train Loss: 3.176982767212477, Val Loss: 20.913873985613694, Test Loss: 3.2541158089818607\n",
            "Epoch 3, Train Loss: 3.126912489029623, Val Loss: 20.58711375464281, Test Loss: 3.203017960524995\n",
            "Epoch 4, Train Loss: 3.0773545813281817, Val Loss: 20.263260405369763, Test Loss: 3.152439879317031\n",
            "Epoch 5, Train Loss: 3.0283177829166052, Val Loss: 19.94187945062871, Test Loss: 3.102359260981997\n",
            "Epoch 6, Train Loss: 2.97979458926921, Val Loss: 19.622939773482493, Test Loss: 3.052791260843328\n",
            "Epoch 7, Train Loss: 2.931785924166462, Val Loss: 19.30640224022393, Test Loss: 3.003711648290893\n",
            "Epoch 8, Train Loss: 2.884273765814288, Val Loss: 18.992324768575095, Test Loss: 2.955128124573465\n",
            "Epoch 9, Train Loss: 2.8372637322581795, Val Loss: 18.680590613354614, Test Loss: 2.907032787108443\n",
            "Epoch 10, Train Loss: 2.7907412522496107, Val Loss: 18.37117921102257, Test Loss: 2.8594140600633513\n",
            "Epoch 11, Train Loss: 2.744693365312791, Val Loss: 18.06404937409445, Test Loss: 2.812279948521986\n",
            "Epoch 12, Train Loss: 2.6991306904412364, Val Loss: 17.759193204144395, Test Loss: 2.7656056087909953\n",
            "Epoch 13, Train Loss: 2.6540385227055157, Val Loss: 17.456522043298094, Test Loss: 2.71942646709702\n",
            "Epoch 14, Train Loss: 2.6094166692365426, Val Loss: 17.156091319634896, Test Loss: 2.673719966531169\n",
            "Epoch 15, Train Loss: 2.565261215972058, Val Loss: 16.857887655932473, Test Loss: 2.6284773580080953\n",
            "Epoch 16, Train Loss: 2.5215620729399713, Val Loss: 16.561825549377417, Test Loss: 2.58369351733435\n",
            "Epoch 17, Train Loss: 2.4783099426176625, Val Loss: 16.26783856358627, Test Loss: 2.5393779509944996\n",
            "Epoch 18, Train Loss: 2.4355105978386704, Val Loss: 15.97619420318147, Test Loss: 2.4955706513760663\n",
            "Epoch 19, Train Loss: 2.3931831255594185, Val Loss: 15.686952890903141, Test Loss: 2.4522185160059293\n",
            "Epoch 20, Train Loss: 2.351299149928813, Val Loss: 15.399862346214404, Test Loss: 2.4093170461390288\n",
            "Epoch 21, Train Loss: 2.309863377800537, Val Loss: 15.114923448338144, Test Loss: 2.3668668887426567\n",
            "Epoch 22, Train Loss: 2.268876051202846, Val Loss: 14.832118234423241, Test Loss: 2.324866973120883\n",
            "Epoch 23, Train Loss: 2.2283326105751096, Val Loss: 14.551439993683381, Test Loss: 2.2833173705425702\n",
            "Epoch 24, Train Loss: 2.1882334825798497, Val Loss: 14.272693675805131, Test Loss: 2.2422086144135744\n",
            "Epoch 25, Train Loss: 2.1485668357471877, Val Loss: 13.996076995386181, Test Loss: 2.2015467617537117\n",
            "Epoch 26, Train Loss: 2.109335062239826, Val Loss: 13.721612658256747, Test Loss: 2.1613313790984696\n",
            "Epoch 27, Train Loss: 2.0705458331669617, Val Loss: 13.449300603432917, Test Loss: 2.1215616878614374\n",
            "Epoch 28, Train Loss: 2.0322010883145003, Val Loss: 13.179242518581605, Test Loss: 2.0822658119573902\n",
            "Epoch 29, Train Loss: 1.9943385035942838, Val Loss: 12.91147135785977, Test Loss: 2.043441394944422\n",
            "Epoch 30, Train Loss: 1.9569418488806494, Val Loss: 12.645877583466108, Test Loss: 2.005079707481363\n",
            "Epoch 31, Train Loss: 1.9199768085747273, Val Loss: 12.382484075034741, Test Loss: 1.9671631612177665\n",
            "Epoch 32, Train Loss: 1.8834568168681036, Val Loss: 12.121327011554675, Test Loss: 1.9296886104495812\n",
            "Epoch 33, Train Loss: 1.8473684562153039, Val Loss: 11.862370777619056, Test Loss: 1.8926644029495248\n",
            "Epoch 34, Train Loss: 1.811704676441652, Val Loss: 11.605736453130698, Test Loss: 1.85609779055239\n",
            "Epoch 35, Train Loss: 1.7764732168482, Val Loss: 11.351843755194988, Test Loss: 1.8199718803884042\n",
            "Epoch 36, Train Loss: 1.7416778389322436, Val Loss: 11.100325597178289, Test Loss: 1.7842859529170862\n",
            "Epoch 37, Train Loss: 1.707312876865623, Val Loss: 10.851503404483072, Test Loss: 1.7490463372076912\n",
            "Epoch 38, Train Loss: 1.6733897830065885, Val Loss: 10.605055479751886, Test Loss: 1.7142436536588963\n",
            "Epoch 39, Train Loss: 1.6398926943676764, Val Loss: 10.360991441290542, Test Loss: 1.6798861565227015\n",
            "Epoch 40, Train Loss: 1.6068435783421, Val Loss: 10.119515084202291, Test Loss: 1.6460058966552649\n",
            "Epoch 41, Train Loss: 1.5742637211997246, Val Loss: 9.880365788725308, Test Loss: 1.6125758856230157\n",
            "Epoch 42, Train Loss: 1.54212830805679, Val Loss: 9.643511045129216, Test Loss: 1.5795978789117233\n",
            "Epoch 43, Train Loss: 1.5104349559257837, Val Loss: 9.409149907728125, Test Loss: 1.5471003540566741\n",
            "Epoch 44, Train Loss: 1.4791809477845732, Val Loss: 9.177260829213893, Test Loss: 1.515054872336035\n",
            "Epoch 45, Train Loss: 1.4483669573185343, Val Loss: 8.947844453020819, Test Loss: 1.4834735015046876\n",
            "Epoch 46, Train Loss: 1.4180025052123355, Val Loss: 8.721069375368252, Test Loss: 1.4523661313627392\n",
            "Epoch 47, Train Loss: 1.3880774208566553, Val Loss: 8.496702992608112, Test Loss: 1.4217164988273827\n",
            "Epoch 48, Train Loss: 1.3586024033327948, Val Loss: 8.274721414896925, Test Loss: 1.3915224714031482\n",
            "Epoch 49, Train Loss: 1.3295761841044618, Val Loss: 8.055001286673326, Test Loss: 1.3617844664613994\n",
            "Epoch 50, Train Loss: 1.3010044935365637, Val Loss: 7.837658085099894, Test Loss: 1.332517432053625\n",
            "Epoch 51, Train Loss: 1.2728934962653515, Val Loss: 7.622779140630538, Test Loss: 1.3037179784724318\n",
            "Epoch 52, Train Loss: 1.2452479354063222, Val Loss: 7.410400975538013, Test Loss: 1.2753838291124775\n",
            "Epoch 53, Train Loss: 1.218054458457892, Val Loss: 7.20059665171304, Test Loss: 1.2475156950750281\n",
            "Epoch 54, Train Loss: 1.1913194626228918, Val Loss: 6.993480554418959, Test Loss: 1.2201351021451603\n",
            "Epoch 55, Train Loss: 1.165045982877905, Val Loss: 6.789065970382483, Test Loss: 1.1932280038307725\n",
            "Epoch 56, Train Loss: 1.139223667096175, Val Loss: 6.587437208353938, Test Loss: 1.166785891028018\n",
            "Epoch 57, Train Loss: 1.1138600175737399, Val Loss: 6.388761928116946, Test Loss: 1.1408242870603829\n",
            "Epoch 58, Train Loss: 1.0889659859511185, Val Loss: 6.1930436654215555, Test Loss: 1.1153356303277255\n",
            "Epoch 59, Train Loss: 1.0645296562704585, Val Loss: 6.000625694465303, Test Loss: 1.090312927990348\n",
            "Epoch 60, Train Loss: 1.0405603591999675, Val Loss: 5.811432512012204, Test Loss: 1.0657769604835843\n",
            "Epoch 61, Train Loss: 1.0170654925177829, Val Loss: 5.625506809311528, Test Loss: 1.0417274899508542\n",
            "Epoch 62, Train Loss: 0.9940395144462699, Val Loss: 5.442840769595905, Test Loss: 1.01816602895837\n",
            "Epoch 63, Train Loss: 0.9714797930269327, Val Loss: 5.263416083437878, Test Loss: 0.9950993130867678\n",
            "Epoch 64, Train Loss: 0.9493796572814783, Val Loss: 5.08720788011609, Test Loss: 0.9725186634469196\n",
            "Epoch 65, Train Loss: 0.9277434107742014, Val Loss: 4.914144838034843, Test Loss: 0.9504225603157\n",
            "Epoch 66, Train Loss: 0.906570172725549, Val Loss: 4.7441132093414815, Test Loss: 0.9288146903257083\n",
            "Epoch 67, Train Loss: 0.88585796807095, Val Loss: 4.577012088316746, Test Loss: 0.9076932880007597\n",
            "Epoch 68, Train Loss: 0.8656118203587361, Val Loss: 4.412884975698546, Test Loss: 0.8870551890883123\n",
            "Epoch 69, Train Loss: 0.845825920509527, Val Loss: 4.251679259535358, Test Loss: 0.8669002923430589\n",
            "Epoch 70, Train Loss: 0.8265012601828555, Val Loss: 4.093396663453134, Test Loss: 0.8472267135348139\n",
            "Epoch 71, Train Loss: 0.8076320825213628, Val Loss: 3.938020376779832, Test Loss: 0.8280311012251467\n",
            "Epoch 72, Train Loss: 0.7892145692744511, Val Loss: 3.78560056177237, Test Loss: 0.8093124420147236\n",
            "Epoch 73, Train Loss: 0.7712494991441138, Val Loss: 3.6361824306483514, Test Loss: 0.7910703774069677\n",
            "Epoch 74, Train Loss: 0.7537357681356899, Val Loss: 3.4898210787002166, Test Loss: 0.7733007161826879\n",
            "Epoch 75, Train Loss: 0.7366688947466651, Val Loss: 3.3464413191452, Test Loss: 0.756001586720716\n",
            "Epoch 76, Train Loss: 0.7200468290893075, Val Loss: 3.206160046082612, Test Loss: 0.7391686014985672\n",
            "Epoch 77, Train Loss: 0.7038670468026823, Val Loss: 3.0690026882243866, Test Loss: 0.7227990142121047\n",
            "Epoch 78, Train Loss: 0.6881253172364039, Val Loss: 2.9350983900677323, Test Loss: 0.7068875529811376\n",
            "Epoch 79, Train Loss: 0.67281849064453, Val Loss: 2.804536053895151, Test Loss: 0.6914355999564122\n",
            "Epoch 80, Train Loss: 0.6579517889257829, Val Loss: 2.6772885820041425, Test Loss: 0.6764367673270797\n",
            "Epoch 81, Train Loss: 0.6435067324559878, Val Loss: 2.553358305312453, Test Loss: 0.6618825329387519\n",
            "Epoch 82, Train Loss: 0.6294806883094451, Val Loss: 2.432743764235055, Test Loss: 0.6477699013087828\n",
            "Epoch 83, Train Loss: 0.6158683164007791, Val Loss: 2.3153853875965025, Test Loss: 0.6340867602124177\n",
            "Epoch 84, Train Loss: 0.6026608675088454, Val Loss: 2.201237559756921, Test Loss: 0.6208269678049539\n",
            "Epoch 85, Train Loss: 0.5898525536882874, Val Loss: 2.090240241623603, Test Loss: 0.607982233898239\n",
            "Epoch 86, Train Loss: 0.5774330534034328, Val Loss: 1.9823518204791168, Test Loss: 0.5955427914753373\n",
            "Epoch 87, Train Loss: 0.5653946809210727, Val Loss: 1.8775668779228527, Test Loss: 0.5835048657798754\n",
            "Epoch 88, Train Loss: 0.553733633047567, Val Loss: 1.7758937045928573, Test Loss: 0.5718598077769225\n",
            "Epoch 89, Train Loss: 0.542442874304931, Val Loss: 1.6773592347814528, Test Loss: 0.5605985709663627\n",
            "Epoch 90, Train Loss: 0.5315135577011506, Val Loss: 1.5820304452656677, Test Loss: 0.5497120603714858\n",
            "Epoch 91, Train Loss: 0.5209394977412826, Val Loss: 1.4899431123584281, Test Loss: 0.5391900839076098\n",
            "Epoch 92, Train Loss: 0.5107108180893009, Val Loss: 1.4011409714923855, Test Loss: 0.529021815750033\n",
            "Epoch 93, Train Loss: 0.5008193556004675, Val Loss: 1.3156414328117088, Test Loss: 0.519199925627767\n",
            "Epoch 94, Train Loss: 0.49125653621819604, Val Loss: 1.2334451940919404, Test Loss: 0.5097169298602781\n",
            "Epoch 95, Train Loss: 0.4820141089986953, Val Loss: 1.1545458174333805, Test Loss: 0.5005636463366077\n",
            "Epoch 96, Train Loss: 0.4730855024522231, Val Loss: 1.0789721216636226, Test Loss: 0.4917324119664952\n",
            "Epoch 97, Train Loss: 0.46446713461483546, Val Loss: 1.0067583212297877, Test Loss: 0.48321111712572384\n",
            "Epoch 98, Train Loss: 0.45614533111503985, Val Loss: 0.9380089273489877, Test Loss: 0.47500001970129874\n",
            "Epoch 99, Train Loss: 0.4481137229961051, Val Loss: 0.8728497472243822, Test Loss: 0.4670768585924117\n",
            "Epoch 100, Train Loss: 0.4403583722689939, Val Loss: 0.8114583313167038, Test Loss: 0.45943433960723085\n",
            "Epoch 101, Train Loss: 0.4328745320866262, Val Loss: 0.7539618413259526, Test Loss: 0.4520592428692067\n",
            "Epoch 102, Train Loss: 0.4256484578962018, Val Loss: 0.7004502512604965, Test Loss: 0.4449425271101029\n",
            "Epoch 103, Train Loss: 0.41867174172129956, Val Loss: 0.6509401035110751, Test Loss: 0.43807533383012826\n",
            "Epoch 104, Train Loss: 0.4119366897498193, Val Loss: 0.6052975595558217, Test Loss: 0.4314454035325366\n",
            "Epoch 105, Train Loss: 0.4054335847904552, Val Loss: 0.5634197602858314, Test Loss: 0.42504693042077085\n",
            "Epoch 106, Train Loss: 0.39915705274468044, Val Loss: 0.525301179525628, Test Loss: 0.41887233730656365\n",
            "Epoch 107, Train Loss: 0.39309577203498125, Val Loss: 0.4908454998908718, Test Loss: 0.41290641274116735\n",
            "Epoch 108, Train Loss: 0.38723876940386553, Val Loss: 0.4598604375520199, Test Loss: 0.4071466705023418\n",
            "Epoch 109, Train Loss: 0.3815839990621562, Val Loss: 0.43212133240284756, Test Loss: 0.40158392179100066\n",
            "Epoch 110, Train Loss: 0.3761217540130063, Val Loss: 0.40734553327498785, Test Loss: 0.3962102629442581\n",
            "Epoch 111, Train Loss: 0.3708448899298607, Val Loss: 0.3852314265397217, Test Loss: 0.39101801192752966\n",
            "Epoch 112, Train Loss: 0.3657460559701407, Val Loss: 0.3655054025339355, Test Loss: 0.3859998029376391\n",
            "Epoch 113, Train Loss: 0.3608184463214871, Val Loss: 0.3479432233540949, Test Loss: 0.38114848751507113\n",
            "Epoch 114, Train Loss: 0.35605579905226376, Val Loss: 0.33236388750669216, Test Loss: 0.37645765001872844\n",
            "Epoch 115, Train Loss: 0.3514517599297048, Val Loss: 0.31861652410285357, Test Loss: 0.3719201373652977\n",
            "Epoch 116, Train Loss: 0.34699939768122445, Val Loss: 0.3065682809028019, Test Loss: 0.36752955135357335\n",
            "Epoch 117, Train Loss: 0.3426927482015229, Val Loss: 0.2960915897445063, Test Loss: 0.36328144608418866\n",
            "Epoch 118, Train Loss: 0.3385284209685609, Val Loss: 0.2870448510426432, Test Loss: 0.35916840758319457\n",
            "Epoch 119, Train Loss: 0.3344990798733384, Val Loss: 0.2792790427114223, Test Loss: 0.35518493126140027\n",
            "Epoch 120, Train Loss: 0.33059943953573256, Val Loss: 0.27263565003963736, Test Loss: 0.3513258338205463\n",
            "Epoch 121, Train Loss: 0.3268225404033807, Val Loss: 0.266958344267544, Test Loss: 0.34758527557192936\n",
            "Epoch 122, Train Loss: 0.3231629357697387, Val Loss: 0.26210154699899296, Test Loss: 0.3439590633365078\n",
            "Epoch 123, Train Loss: 0.3196166945232572, Val Loss: 0.25793492977717036, Test Loss: 0.3404445044532946\n",
            "Epoch 124, Train Loss: 0.3161790731255292, Val Loss: 0.25434666360436425, Test Loss: 0.33703384988484203\n",
            "Epoch 125, Train Loss: 0.3128456898491552, Val Loss: 0.25124361508126714, Test Loss: 0.33372327035176197\n",
            "Epoch 126, Train Loss: 0.3096134799836275, Val Loss: 0.24854902390619837, Test Loss: 0.3305081812367216\n",
            "Epoch 127, Train Loss: 0.30647706993865, Val Loss: 0.24620066362343473, Test Loss: 0.32738464037470877\n",
            "Epoch 128, Train Loss: 0.303432544784962, Val Loss: 0.24414795512374846, Test Loss: 0.32434888704429654\n",
            "Epoch 129, Train Loss: 0.3004761585911966, Val Loss: 0.24234979502235923, Test Loss: 0.32139733472599075\n",
            "Epoch 130, Train Loss: 0.29760463470670795, Val Loss: 0.24077254940123816, Test Loss: 0.3185270539401268\n",
            "Epoch 131, Train Loss: 0.294815258203876, Val Loss: 0.2393885451354901, Test Loss: 0.3157345485370629\n",
            "Epoch 132, Train Loss: 0.2921044292382508, Val Loss: 0.23817436666926892, Test Loss: 0.3130192115905384\n",
            "Epoch 133, Train Loss: 0.28947014041096747, Val Loss: 0.23711097396616146, Test Loss: 0.3103766551277416\n",
            "Epoch 134, Train Loss: 0.2869086933143248, Val Loss: 0.2361832799115311, Test Loss: 0.30780249686693517\n",
            "Epoch 135, Train Loss: 0.2844159597241695, Val Loss: 0.23537736355189673, Test Loss: 0.30529463385741434\n",
            "Epoch 136, Train Loss: 0.28198999377331013, Val Loss: 0.23468219689705586, Test Loss: 0.30284993552948225\n",
            "Epoch 137, Train Loss: 0.27962736339520217, Val Loss: 0.23408793679340667, Test Loss: 0.3004659708770058\n",
            "Epoch 138, Train Loss: 0.27732560047712806, Val Loss: 0.23358605228567528, Test Loss: 0.29813945962276994\n",
            "Epoch 139, Train Loss: 0.27508262490228097, Val Loss: 0.23316888309933767, Test Loss: 0.2958695984064128\n",
            "Epoch 140, Train Loss: 0.27289638739069483, Val Loss: 0.2328300248080522, Test Loss: 0.29365384382849574\n",
            "Epoch 141, Train Loss: 0.27076420876701474, Val Loss: 0.23256368822054518, Test Loss: 0.2914901586549625\n",
            "Epoch 142, Train Loss: 0.26868402310821976, Val Loss: 0.23236469252140884, Test Loss: 0.2893766625460065\n",
            "Epoch 143, Train Loss: 0.2666539219539941, Val Loss: 0.2322283816629592, Test Loss: 0.2873115318964094\n",
            "Epoch 144, Train Loss: 0.26467185710651536, Val Loss: 0.2321506777938238, Test Loss: 0.28529280987208494\n",
            "Epoch 145, Train Loss: 0.26273576457731035, Val Loss: 0.23212627927308735, Test Loss: 0.2833194919022936\n",
            "Epoch 146, Train Loss: 0.2608438548560471, Val Loss: 0.23215303085362904, Test Loss: 0.2813898242383781\n",
            "Epoch 147, Train Loss: 0.25899511419764953, Val Loss: 0.23222771479172913, Test Loss: 0.2795020000491284\n",
            "Epoch 148, Train Loss: 0.25718799749172216, Val Loss: 0.2323473587838582, Test Loss: 0.2776545766022738\n",
            "Epoch 149, Train Loss: 0.2554210293506329, Val Loss: 0.23250920498560354, Test Loss: 0.27584610409014426\n",
            "Epoch 150, Train Loss: 0.2536928006870904, Val Loss: 0.23271068406399664, Test Loss: 0.2740753090638337\n",
            "Epoch 151, Train Loss: 0.25200196544813147, Val Loss: 0.23294939339969212, Test Loss: 0.2723409632059887\n",
            "Epoch 152, Train Loss: 0.2503472375012912, Val Loss: 0.23322307872645148, Test Loss: 0.27064186348236563\n",
            "Epoch 153, Train Loss: 0.2487273876683819, Val Loss: 0.2335296186307746, Test Loss: 0.2689768593424112\n",
            "Epoch 154, Train Loss: 0.24714127458192878, Val Loss: 0.23386683088327806, Test Loss: 0.26734493531637066\n",
            "Epoch 155, Train Loss: 0.2455877969687891, Val Loss: 0.23423324017971392, Test Loss: 0.26574484318909414\n",
            "Epoch 156, Train Loss: 0.24406573185743327, Val Loss: 0.23462682736901308, Test Loss: 0.26417568099009875\n",
            "Epoch 157, Train Loss: 0.24257416104564514, Val Loss: 0.2350458394797959, Test Loss: 0.26263651601531135\n",
            "Epoch 158, Train Loss: 0.2411121240062172, Val Loss: 0.23548870672493544, Test Loss: 0.26112682840821616\n",
            "Epoch 159, Train Loss: 0.23967863099768352, Val Loss: 0.2359538591134555, Test Loss: 0.259645431663822\n",
            "Epoch 160, Train Loss: 0.23827287286636892, Val Loss: 0.23643992969631913, Test Loss: 0.25819122861664007\n",
            "Epoch 161, Train Loss: 0.2368937763509632, Val Loss: 0.23694545319636504, Test Loss: 0.2567634930994425\n",
            "Epoch 162, Train Loss: 0.2355406330919169, Val Loss: 0.237469097133069, Test Loss: 0.25536232210046916\n",
            "Epoch 163, Train Loss: 0.23421264922812862, Val Loss: 0.23800959118073461, Test Loss: 0.25398683923208054\n",
            "Epoch 164, Train Loss: 0.23290920150199212, Val Loss: 0.23856558562166053, Test Loss: 0.2526357640090405\n",
            "Epoch 165, Train Loss: 0.23162957818718855, Val Loss: 0.23913604365010785, Test Loss: 0.2513081527904465\n",
            "Epoch 166, Train Loss: 0.23037290694520837, Val Loss: 0.23971986699474826, Test Loss: 0.25000334246045147\n",
            "Epoch 167, Train Loss: 0.22913850452453136, Val Loss: 0.24031599733317616, Test Loss: 0.2487206825290679\n",
            "Epoch 168, Train Loss: 0.22792571114879, Val Loss: 0.2409234262506248, Test Loss: 0.24745957225500337\n",
            "Epoch 169, Train Loss: 0.2267339483506011, Val Loss: 0.24154120589294883, Test Loss: 0.24621945902854597\n",
            "Epoch 170, Train Loss: 0.22556267573823138, Val Loss: 0.24216847002778152, Test Loss: 0.24499973571322886\n",
            "Epoch 171, Train Loss: 0.22441124470338708, Val Loss: 0.2428043645056506, Test Loss: 0.24379985609285248\n",
            "Epoch 172, Train Loss: 0.2232791051053755, Val Loss: 0.24344807924209738, Test Loss: 0.24261929411644445\n",
            "Epoch 173, Train Loss: 0.2221659416530927, Val Loss: 0.24409881949515433, Test Loss: 0.24145770003374686\n",
            "Epoch 174, Train Loss: 0.2210712711258409, Val Loss: 0.24475590078972945, Test Loss: 0.2403144197368342\n",
            "Epoch 175, Train Loss: 0.21999464461744034, Val Loss: 0.24541682923124994, Test Loss: 0.2391894973406265\n",
            "Epoch 176, Train Loss: 0.21893576980184512, Val Loss: 0.2460827493740986, Test Loss: 0.23808251425231894\n",
            "Epoch 177, Train Loss: 0.21789367846472593, Val Loss: 0.24675303504724083, Test Loss: 0.2369926062635075\n",
            "Epoch 178, Train Loss: 0.2168679291297098, Val Loss: 0.24742712807381956, Test Loss: 0.23591925693981936\n",
            "Epoch 179, Train Loss: 0.21585813483275973, Val Loss: 0.24810051250671614, Test Loss: 0.23486202613379287\n",
            "Epoch 180, Train Loss: 0.21486392491597184, Val Loss: 0.24877669836618085, Test Loss: 0.23382047626514577\n",
            "Epoch 181, Train Loss: 0.21388487435323134, Val Loss: 0.24945467189544054, Test Loss: 0.232794348134211\n",
            "Epoch 182, Train Loss: 0.21292073664693825, Val Loss: 0.25013452439956835, Test Loss: 0.2317831492771498\n",
            "Epoch 183, Train Loss: 0.21197106101901378, Val Loss: 0.25081597113247356, Test Loss: 0.23078675908839125\n",
            "Epoch 184, Train Loss: 0.2110357601688173, Val Loss: 0.2514984811255592, Test Loss: 0.22980458986611796\n",
            "Epoch 185, Train Loss: 0.21011414853011953, Val Loss: 0.2521816911326533, Test Loss: 0.22883629859189283\n",
            "Epoch 186, Train Loss: 0.2092059192430826, Val Loss: 0.25286525935643595, Test Loss: 0.2278815833733392\n",
            "Epoch 187, Train Loss: 0.20831072479112095, Val Loss: 0.2535481210239668, Test Loss: 0.226940134826787\n",
            "Epoch 188, Train Loss: 0.20742825894122136, Val Loss: 0.25423051308901123, Test Loss: 0.22601165287492564\n",
            "Epoch 189, Train Loss: 0.20655822550910163, Val Loss: 0.25491242730003855, Test Loss: 0.22509584687245773\n",
            "Epoch 190, Train Loss: 0.20570033794023837, Val Loss: 0.25559360987482466, Test Loss: 0.22419243522708265\n",
            "Epoch 191, Train Loss: 0.20485431891146855, Val Loss: 0.2562738261118663, Test Loss: 0.22330114503873127\n",
            "Epoch 192, Train Loss: 0.20401989995204586, Val Loss: 0.2569528593385918, Test Loss: 0.22242171175605574\n",
            "Epoch 193, Train Loss: 0.20319682108309833, Val Loss: 0.25763050989605096, Test Loss: 0.22155387884923655\n",
            "Epoch 194, Train Loss: 0.20238483047449132, Val Loss: 0.2583065941655755, Test Loss: 0.22069739749822595\n",
            "Epoch 195, Train Loss: 0.20158360918351662, Val Loss: 0.2589808652063563, Test Loss: 0.21985194556876003\n",
            "Epoch 196, Train Loss: 0.20079291252808973, Val Loss: 0.25965324204587376, Test Loss: 0.2190173735382945\n",
            "Epoch 197, Train Loss: 0.2000127195423531, Val Loss: 0.2603236367584257, Test Loss: 0.21819376465829335\n",
            "Epoch 198, Train Loss: 0.19924304959468064, Val Loss: 0.26099185617369824, Test Loss: 0.21738057999565608\n",
            "Epoch 199, Train Loss: 0.19848331667044725, Val Loss: 0.26165778411111895, Test Loss: 0.21657760508300647\n",
            "Epoch 200, Train Loss: 0.19773331142318504, Val Loss: 0.2623213154058035, Test Loss: 0.21578463157789407\n",
            "Epoch 201, Train Loss: 0.19699285449294776, Val Loss: 0.2629825458991224, Test Loss: 0.21500160510818297\n",
            "Epoch 202, Train Loss: 0.1962620819424599, Val Loss: 0.2636403886112743, Test Loss: 0.2142283102355147\n",
            "Epoch 203, Train Loss: 0.19554055428582914, Val Loss: 0.2642956077863021, Test Loss: 0.2134644248256691\n",
            "Epoch 204, Train Loss: 0.1948280692417626, Val Loss: 0.2649470252773773, Test Loss: 0.21270991247839127\n",
            "Epoch 205, Train Loss: 0.19412444457655195, Val Loss: 0.2655956630749558, Test Loss: 0.21196442983239186\n",
            "Epoch 206, Train Loss: 0.19342939506344498, Val Loss: 0.2662414691072625, Test Loss: 0.21122780138022015\n",
            "Epoch 207, Train Loss: 0.1927427505307123, Val Loss: 0.2668843979295011, Test Loss: 0.21049985634207316\n",
            "Epoch 208, Train Loss: 0.1920643455267667, Val Loss: 0.2675244105483886, Test Loss: 0.20978042850189996\n",
            "Epoch 209, Train Loss: 0.19139401915085777, Val Loss: 0.2681614738395259, Test Loss: 0.2090693560504534\n",
            "Epoch 210, Train Loss: 0.19073161489115462, Val Loss: 0.26879556009324895, Test Loss: 0.2083664814349463\n",
            "Epoch 211, Train Loss: 0.19007698046984403, Val Loss: 0.2694266466198992, Test Loss: 0.20767165121498735\n",
            "Epoch 212, Train Loss: 0.18942996769489248, Val Loss: 0.27005471537911047, Test Loss: 0.2069847159244896\n",
            "Epoch 213, Train Loss: 0.18879044079649207, Val Loss: 0.2706794298442929, Test Loss: 0.20630548654736094\n",
            "Epoch 214, Train Loss: 0.188158297758716, Val Loss: 0.2713011093119084, Test Loss: 0.20563386440393153\n",
            "Epoch 215, Train Loss: 0.1875333878488703, Val Loss: 0.2719195224453223, Test Loss: 0.20496975285957858\n",
            "Epoch 216, Train Loss: 0.18691556740082393, Val Loss: 0.2725353720333505, Test Loss: 0.2043129486713805\n",
            "Epoch 217, Train Loss: 0.1863046557762147, Val Loss: 0.273148173047838, Test Loss: 0.20366334663781827\n",
            "Epoch 218, Train Loss: 0.18570057088105618, Val Loss: 0.2737570918175053, Test Loss: 0.20302103796978102\n",
            "Epoch 219, Train Loss: 0.18510340899439168, Val Loss: 0.2743624488808221, Test Loss: 0.20238578875077665\n",
            "Epoch 220, Train Loss: 0.1845129926428895, Val Loss: 0.2749652745112442, Test Loss: 0.2017574667475621\n",
            "Epoch 221, Train Loss: 0.18392910065892445, Val Loss: 0.27556501789813687, Test Loss: 0.20113585025230019\n",
            "Epoch 222, Train Loss: 0.18335160875569032, Val Loss: 0.2761615329947331, Test Loss: 0.2005211338677082\n",
            "Epoch 223, Train Loss: 0.18278068320786262, Val Loss: 0.27675494310135124, Test Loss: 0.19991283936441653\n",
            "Epoch 224, Train Loss: 0.18221576650590707, Val Loss: 0.2773453738148295, Test Loss: 0.19931089495518134\n",
            "Epoch 225, Train Loss: 0.18165683672907076, Val Loss: 0.27793285185262184, Test Loss: 0.19871519417474126\n",
            "Epoch 226, Train Loss: 0.18110379734585705, Val Loss: 0.27851739639316264, Test Loss: 0.19812575218146\n",
            "Epoch 227, Train Loss: 0.18055654771506494, Val Loss: 0.2790990278320908, Test Loss: 0.19754269565476157\n",
            "Epoch 228, Train Loss: 0.18001505874557244, Val Loss: 0.2796788601365731, Test Loss: 0.19696585793589932\n",
            "Epoch 229, Train Loss: 0.17947957922034768, Val Loss: 0.2802558094597749, Test Loss: 0.1963948517625404\n",
            "Epoch 230, Train Loss: 0.17894960392450474, Val Loss: 0.2808298992719017, Test Loss: 0.1958295814011948\n",
            "Epoch 231, Train Loss: 0.1784250412361491, Val Loss: 0.28140115382346936, Test Loss: 0.19526995328493615\n",
            "Epoch 232, Train Loss: 0.17790580164643968, Val Loss: 0.2819695980529408, Test Loss: 0.194715875950605\n",
            "Epoch 233, Train Loss: 0.17739179769659033, Val Loss: 0.2825352575010898, Test Loss: 0.19416725997824963\n",
            "Epoch 234, Train Loss: 0.17688294404422047, Val Loss: 0.2830982047800033, Test Loss: 0.19362401848024682\n",
            "Epoch 235, Train Loss: 0.17637916042955243, Val Loss: 0.2836584187772719, Test Loss: 0.19308606552989901\n",
            "Epoch 236, Train Loss: 0.17588036187922365, Val Loss: 0.28421592640982923, Test Loss: 0.19255331748973387\n",
            "Epoch 237, Train Loss: 0.17538646854458842, Val Loss: 0.28477075491300907, Test Loss: 0.19202569254399102\n",
            "Epoch 238, Train Loss: 0.1748974023451387, Val Loss: 0.2853229317824667, Test Loss: 0.19150311064790848\n",
            "Epoch 239, Train Loss: 0.17441308691799276, Val Loss: 0.2858724847205769, Test Loss: 0.1909854934787432\n",
            "Epoch 240, Train Loss: 0.17393344756916634, Val Loss: 0.2864194415870011, Test Loss: 0.1904727643884541\n",
            "Epoch 241, Train Loss: 0.17345843554947724, Val Loss: 0.2869632505502817, Test Loss: 0.18996484660642618\n",
            "Epoch 242, Train Loss: 0.1729880229501257, Val Loss: 0.28750452974211266, Test Loss: 0.189461671399572\n",
            "Epoch 243, Train Loss: 0.1725220763735432, Val Loss: 0.2880433011489501, Test Loss: 0.1889631642107021\n",
            "Epoch 244, Train Loss: 0.17206051265118188, Val Loss: 0.28857934107801414, Test Loss: 0.1884692553350186\n",
            "Epoch 245, Train Loss: 0.17160325294175416, Val Loss: 0.2891129330525788, Test Loss: 0.1879798750243107\n",
            "Epoch 246, Train Loss: 0.17115025373762643, Val Loss: 0.28964410487377834, Test Loss: 0.18749495576566275\n",
            "Epoch 247, Train Loss: 0.17070145079580235, Val Loss: 0.29017288425243076, Test Loss: 0.187014431430865\n",
            "Epoch 248, Train Loss: 0.17025678120796287, Val Loss: 0.2906992987842778, Test Loss: 0.1865382372402412\n",
            "Epoch 249, Train Loss: 0.16981618336484305, Val Loss: 0.2912233759274244, Test Loss: 0.18606630972763277\n",
            "Epoch 250, Train Loss: 0.16937960521120604, Val Loss: 0.2917445351416678, Test Loss: 0.18559866688170362\n",
            "Epoch 251, Train Loss: 0.16894708039994435, Val Loss: 0.29226342719515885, Test Loss: 0.18513521025811847\n",
            "Epoch 252, Train Loss: 0.16851844469242563, Val Loss: 0.2927800680003195, Test Loss: 0.18467583577979568\n",
            "Epoch 253, Train Loss: 0.16809364400489776, Val Loss: 0.29329448425856874, Test Loss: 0.1842206150174864\n",
            "Epoch 254, Train Loss: 0.16767262280192988, Val Loss: 0.2938067024467784, Test Loss: 0.18376937884272443\n",
            "Epoch 255, Train Loss: 0.1672553266542392, Val Loss: 0.29431674880516673, Test Loss: 0.1833220489908623\n",
            "Epoch 256, Train Loss: 0.1668417022104606, Val Loss: 0.29482464932649455, Test Loss: 0.18287857655411305\n",
            "Epoch 257, Train Loss: 0.16643172075500448, Val Loss: 0.29533030746565614, Test Loss: 0.18243897305489354\n",
            "Epoch 258, Train Loss: 0.166025347118083, Val Loss: 0.29583387376304243, Test Loss: 0.18200310944498402\n",
            "Epoch 259, Train Loss: 0.16562249043214167, Val Loss: 0.2963356312682447, Test Loss: 0.1815709476829913\n",
            "Epoch 260, Train Loss: 0.1652231364518658, Val Loss: 0.2968353445153862, Test Loss: 0.1811424252302171\n",
            "Epoch 261, Train Loss: 0.16482720054486377, Val Loss: 0.2973330381258439, Test Loss: 0.1807174921797132\n",
            "Epoch 262, Train Loss: 0.16443464063341906, Val Loss: 0.29782882046915204, Test Loss: 0.18029612311889764\n",
            "Epoch 263, Train Loss: 0.1640454821088498, Val Loss: 0.2983223568745786, Test Loss: 0.17987826637684898\n",
            "Epoch 264, Train Loss: 0.16365969343981712, Val Loss: 0.2988139582618617, Test Loss: 0.17946385319066624\n",
            "Epoch 265, Train Loss: 0.16327713518026818, Val Loss: 0.2993036545786898, Test Loss: 0.17905283733345084\n",
            "Epoch 266, Train Loss: 0.16289776350706478, Val Loss: 0.29979146190413497, Test Loss: 0.17864517343207054\n",
            "Epoch 267, Train Loss: 0.16252153541302392, Val Loss: 0.3002774024988106, Test Loss: 0.17824081694717442\n",
            "Epoch 268, Train Loss: 0.16214840868754468, Val Loss: 0.30076149857635603, Test Loss: 0.1778397241537756\n",
            "Epoch 269, Train Loss: 0.16177834189780055, Val Loss: 0.30124377205401154, Test Loss: 0.17744185212238556\n",
            "Epoch 270, Train Loss: 0.1614112943704765, Val Loss: 0.30172424455233443, Test Loss: 0.17704716339565776\n",
            "Epoch 271, Train Loss: 0.1610472261740325, Val Loss: 0.30220293739531007, Test Loss: 0.17665563652714386\n",
            "Epoch 272, Train Loss: 0.16068609810147513, Val Loss: 0.30267987161082305, Test Loss: 0.17626720528164877\n",
            "Epoch 273, Train Loss: 0.1603278716536197, Val Loss: 0.30315506793146074, Test Loss: 0.17588182975895464\n",
            "Epoch 274, Train Loss: 0.1599725090228268, Val Loss: 0.30362854679561874, Test Loss: 0.17549947076692277\n",
            "Epoch 275, Train Loss: 0.1596199730771966, Val Loss: 0.3041003283488827, Test Loss: 0.17512008980558483\n",
            "Epoch 276, Train Loss: 0.15927022925183504, Val Loss: 0.3045703277599132, Test Loss: 0.17474366390794877\n",
            "Epoch 277, Train Loss: 0.15892326612037058, Val Loss: 0.30503867029881987, Test Loss: 0.17437014056369982\n",
            "Epoch 278, Train Loss: 0.15857902153417666, Val Loss: 0.30550537523690546, Test Loss: 0.17399948327193537\n",
            "Epoch 279, Train Loss: 0.1582374609315687, Val Loss: 0.3059704615602567, Test Loss: 0.17363165616407997\n",
            "Epoch 280, Train Loss: 0.15789855035344583, Val Loss: 0.30643394797218665, Test Loss: 0.17326662399002424\n",
            "Epoch 281, Train Loss: 0.15756225642994537, Val Loss: 0.30689585289582566, Test Loss: 0.17290435210463273\n",
            "Epoch 282, Train Loss: 0.15722854636745828, Val Loss: 0.30735619447684903, Test Loss: 0.17254480645460837\n",
            "Epoch 283, Train Loss: 0.15689740565384555, Val Loss: 0.307814390795989, Test Loss: 0.17218816208817705\n",
            "Epoch 284, Train Loss: 0.1565690096276427, Val Loss: 0.3082710563676988, Test Loss: 0.17183424459626154\n",
            "Epoch 285, Train Loss: 0.15624309729987726, Val Loss: 0.30872620869255973, Test Loss: 0.17148294873110126\n",
            "Epoch 286, Train Loss: 0.15591963816384743, Val Loss: 0.3091798650001982, Test Loss: 0.1711342428017862\n",
            "Epoch 287, Train Loss: 0.15559860222691355, Val Loss: 0.30963204225265006, Test Loss: 0.17078809564586053\n",
            "Epoch 288, Train Loss: 0.15527995999950484, Val Loss: 0.31008275714778216, Test Loss: 0.17044447661817366\n",
            "Epoch 289, Train Loss: 0.15496368248441394, Val Loss: 0.3105320261227603, Test Loss: 0.17010335558001544\n",
            "Epoch 290, Train Loss: 0.15464974116636898, Val Loss: 0.3109798653575545, Test Loss: 0.16976470288852843\n",
            "Epoch 291, Train Loss: 0.1543381080018754, Val Loss: 0.31142629077847755, Test Loss: 0.1694284893863881\n",
            "Epoch 292, Train Loss: 0.1540287554093192, Val Loss: 0.3118713180617459, Test Loss: 0.16909476859421205\n",
            "Epoch 293, Train Loss: 0.15372165625932313, Val Loss: 0.31231496263705755, Test Loss: 0.16876346793576172\n",
            "Epoch 294, Train Loss: 0.1534167838653485, Val Loss: 0.31275723969118063, Test Loss: 0.1684345198752862\n",
            "Epoch 295, Train Loss: 0.15311411522758833, Val Loss: 0.31319814368702936, Test Loss: 0.1681079014749206\n",
            "Epoch 296, Train Loss: 0.1528136305053932, Val Loss: 0.3136377231087085, Test Loss: 0.16778358339526345\n",
            "Epoch 297, Train Loss: 0.15251529803720776, Val Loss: 0.3140759792179191, Test Loss: 0.16746153717697\n",
            "Epoch 298, Train Loss: 0.15221908951166457, Val Loss: 0.31451292626729005, Test Loss: 0.16714173682959724\n",
            "Epoch 299, Train Loss: 0.1519249803298086, Val Loss: 0.31494857828431483, Test Loss: 0.16682418722292638\n",
            "Epoch 300, Train Loss: 0.15163294628374163, Val Loss: 0.3153829490749071, Test Loss: 0.16650884181310815\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = fit(X_train, y_train, X_val, y_val, X_test, y_test, weights, biases, epochs, learning_rate, batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c41885",
      "metadata": {
        "id": "45c41885"
      },
      "source": [
        "#### Display the training loss and validation loss against epoch graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "id": "47f05472",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "47f05472",
        "outputId": "2a1f98ce-3ead-49e6-cb9f-5ba4e2d84e15"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Plot the validation and test loss over epochs\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.plot(history['test_loss'], label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation and Test Loss')\n",
        "plt.legend()\n",
        "plt.xticks(ticks=[0, 50, 100, 150, 200,250,300])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0b4c38",
      "metadata": {
        "id": "0c0b4c38"
      },
      "source": [
        "#### Predict the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "id": "a84f73b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a84f73b1",
        "outputId": "7ac0a414-a966-46d9-864b-23de9c8c3f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def predict(X, weights, biases):\n",
        "  #calculate logits using forward pass\n",
        "\n",
        "    logits = forward(X, weights, biases)\n",
        "   #apply sigmoid to logits\n",
        "    return tf.round(tf.sigmoid(logits))\n",
        "\n",
        "\n",
        "y_pred = predict(X_test, weights, biases).numpy()\n",
        "#print predicted labels\n",
        "print(y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d715ef",
      "metadata": {
        "id": "20d715ef"
      },
      "source": [
        "#### Display the confusion matrix and the classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "id": "c35deeb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c35deeb3",
        "outputId": "98bff33d-3aa0-4885-f2bf-dca15a2e6b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[95  5]\n",
            " [ 3 97]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.97      0.95      0.96       100\n",
            "     Class 1       0.95      0.97      0.96       100\n",
            "\n",
            "    accuracy                           0.96       200\n",
            "   macro avg       0.96      0.96      0.96       200\n",
            "weighted avg       0.96      0.96      0.96       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Print the classification report\n",
        "cr = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])\n",
        "print(\"Classification Report:\")\n",
        "print(cr)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
